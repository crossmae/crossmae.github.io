<html>

<head>
    <meta charset="utf-8" />
    <title>CrossMAE: Rethinking Patch Dependence for Masked Autoencoders</title>

    <meta content="CrossMAE: Rethinking Patch Dependence for Masked Autoencoders" name="description" />
    <meta content="CrossMAE: Rethinking Patch Dependence for Masked Autoencoders" property="og:title" />
    <meta content="CrossMAE: Rethinking Patch Dependence for Masked Autoencoders" property="og:description" />
    <meta content="https://crossmae.github.io/crossmae2.jpg" property="og:image" />
    <meta content="CrossMAE: Rethinking Patch Dependence for Masked Autoencoders" property="twitter:title" />
    <meta content="CrossMAE: Rethinking Patch Dependence for Masked Autoencoders" property="twitter:description" />
    <meta content="https://crossmae.github.io/crossmae2.jpg" property="twitter:image" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css?v=v2" rel="stylesheet" type="text/css" />
</head>

<body>
    <div class="section">
        <div class="container">
            <div class="title-row">
                <h1 class="title main-title"
                    style="font-weight: 700; font-size: 50px; font-family: 'Varela Round',sans-serif; margin: 0">
                    CrossMAE</h1>
                <h1 class="title main-title">Rethinking Patch Dependence for Masked Autoencoders</h1>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col1">
                    <a href="https://max-fu.github.io" target="_blank" class="author-text">Letian Fu</a><sup>1*</sup>
                </div>
                <div class="base-col author-col1">
                    <a href="https://tonylian.com" target="_blank" class="author-text">Long Lian</a><sup>1*</sup>
                </div>
                <div class="base-col author-col1">
                    <a href="https://renwang435.github.io" target="_blank" class="author-text">Renhao
                        Wang</a><sup>1</sup>
                </div>
                <div class="base-col author-col1">
                    <a href="https://bfshi.github.io" target="_blank" class="author-text">Baifeng Shi</a><sup>1</sup>
                </div>
                <div class="base-col author-col1">
                    <a href="https://people.eecs.berkeley.edu/~xdwang" target="_blank" class="author-text">Xudong
                        Wang</a><sup>1</sup>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col2">
                    <a href="https://www.adamyala.org" target="_blank" class="author-text">Adam Yala</a><sup>1,2†</sup>
                </div>
                <div class="base-col author-col2">
                    <a href="https://people.eecs.berkeley.edu/~trevor" target="_blank" class="author-text">Trevor
                        Darrell</a><sup>1†</sup>
                </div>
                <div class="base-col author-col2">
                    <a href="https://people.eecs.berkeley.edu/~efros" target="_blank" class="author-text">Alexei A.
                        Efros</a><sup>1†</sup>
                </div>
                <div class="base-col author-col2">
                    <a href="https://goldberg.berkeley.edu" target="_blank" class="author-text">Ken
                        Goldberg</a><sup>1†</sup>
                </div>
            </div>
            <div class="base-row affiliation-row">
                <div class="base-col affiliation-col">
                    <sup>1</sup>UC Berkeley
                </div>
                <div class="base-col affiliation-col">
                    <sup>2</sup>UCSF
                </div>
                <div class="base-col affiliation-col"><sup>*</sup>Equal Contribution</div>
                <div class="base-col affiliation-col"><sup>†</sup>Equal Advising</div>
            </div>
            <div class="link-labels base-row" style="max-width: 500px">
                <div class="base-col icon-col"><a href="https://crossmae.github.io/assets/crossmae.pdf" target="_blank"
                        class="link-block">
                        <i class="fa fas fa-file-text main-icon" style="font-size: 50px"></i>
                    </a></div>
                <div class="base-col icon-col"><a href="https://github.com/TonyLianLong/CrossMAE" class="link-block">
                        <i class="fa fa-github main-icon" style="font-size: 55px"></i>
                    </a></div>
                <div class="base-col icon-col"><a href="#citation" class="link-block">
                        <i class="fa fa-graduation-cap main-icon" style="font-size: 55px"></i>
                    </a></div>
            </div>
            <div class="link-labels base-row" style="max-width: 500px">
                <div class="base-col icon-col"><a href="https://crossmae.github.io/assets/crossmae.pdf" target="_blank"
                        class="no-underline">
                        <strong class="link-labels-text">Paper</strong></a>
                </div>
                <div class="base-col icon-col"><a href="https://github.com/TonyLianLong/CrossMAE" class="no-underline">
                        <strong class="link-labels-text">Code</strong></a>
                </div>
                <div class="base-col icon-col"><a href="#citation" class="no-underline">
                        <strong class="link-labels-text">Citation</strong></a>
                </div>
            </div>
            <h1 class="tldr">
                <b>TL;DR</b>: Independent partial patch reconstruction facilitates efficient representation learning.
            </h1>

            <div class="base-row add-top-padding">
                <div class="base-row add-top-padding">
                    <img class="img" src="crossmae1.jpg" />
                </div>
                <h1 class="title">Overview</h1>
                <p class="paragraph">
                    We introduce <b>Cross-Attention Masked Autoencoders (CrossMAE)</b>, which use only cross-attention
                    for decoding in MAE. We show that CrossMAE greatly enhances efficiency and performance in tasks like
                    ImageNet classification and COCO instance segmentation, with significantly reduced computational
                    demands.
                </p>
            </div>
            <div class="base-row add-large-top-padding">
                <div class="base-row add-top-padding">
                    <img class="img" src="crossmae2.jpg" />
                </div>
                <p class="paragraph add-top-padding">MAE concatenates all mask tokens with the visible patch features
                    from a ViT encoder and passes them to a decoder with self-attention blocks to reconstruct the
                    original image. Patches that correspond to visible tokens are then dropped, and an L2 loss is
                    applied to the rest of the reconstruction as the pretraining objective. <b>CrossMAE instead uses
                        cross-attention blocks in the decoder to reconstruct only a subset of the masked tokens.</b></p>
            </div>
            <div class="base-row add-large-top-padding">
                <h1 class="title">Cross-Attention, Partial Masking, and Inter-block Attention</h1>
                <div class="base-row add-top-padding">
                    <img class="img" src="crossmae3.jpg" />
                </div>
                <p class="paragraph add-top-padding"><b>Overview of CrossMAE.</b> (a) The vanilla version of CrossMAE
                    uses the output of the last encoder block as the keys and queries for cross-attention. The first
                    decoder block takes the sum of mask tokens and their corresponding positional embeddings as queries,
                    and subsequent layers use the output of the previous decoder block as queries to reconstruct the
                    masked patches. (b) Unlike the decoder block in Transformers, the cross-attention decoder block does
                    not contain self-attention, decoupling the generation of different masked patches. (c) CrossMAE's
                    decoder blocks can leverage low-level features for reconstruction via inter-block attention. It
                    weighs the intermediate feature maps, and the weighted sum of feature maps is used as the key and
                    value for each decoder block.</p>
            </div>
            <div class="base-row add-top-padding">
                <h1 class="title">Visualizations</h1>
                <div class="base-row add-top-padding">
                    <img class="img" src="crossmae4.jpg" />
                </div>
                <p class="paragraph">
                    <b>Example reconstructions of ImageNet validation images.</b> For each set of 5 images, from left to
                    right, are the original image, masked image with a mask ratio of 75%, MAE, CrossMAE (trained to
                    reconstruct 25% of image tokens, or 1/3 of the mask tokens), and CrossMAE (trained to reconstruct
                    all masked tokens). Since CrossMAE does not reconstruct them, all model outputs have the visible
                    patches overlaid. Intriguingly, CrossMAE, when trained for partial reconstruction, can decode all
                    mask tokens in one forward pass (shown above), which deviates from its training methodology. Its
                    comparable reconstruction quality to full-image-trained models suggests that full-image
                    reconstruction might not be essential for effective representation learning.
                </p>
            </div>
            <div class="base-row add-top-padding">
                <h1 class="title">Results</h1>
                <div class="base-row add-large-top-padding">
                    <img class="img" src="crossmae6.jpg" style="max-width:800px; width: 50%" />
                </div>
                <div class="base-row add-large-top-padding">
                    <img class="img" src="crossmae7.jpg" style="max-width:800px; width: 50%" />
                </div>
                <div class="base-row add-large-top-padding">
                    <img class="img" src="crossmae8.jpg" style="max-width:800px; width: 50%" />
                </div>
            </div>
            <!-- <div class="base-row add-large-padding">
                <h1 class="title">Analysis</h1>
                <div class="base-row add-top-padding">
                    <img class="img" src="crossmae5.jpg" />
                </div>
                <p class="paragraph">
                    We visualize the output of each decoder block. (a-b) Different decoder blocks play different roles in the reconstruction, with most details emerging at later decoder blocks, which confirms the motivation for inter-block attention. (c) Visualizations of inter-block attention shows that different decoder blocks indeed attend to feature from different encoder blocks, with later blocks focusing on earlier encoder features to achieve reconstruction.
                </p>
            </div> -->

            <div class="citation add-large-top-padding">
                <h1 id="citation">Citation</h1>
                <p style="font-size: 16px">If you use this work or find it helpful, please consider citing our work.</p>
                <!-- <pre id="codecell0">

                </pre> -->
            </div>
        </div>
    </div>

    <p class="credit">Credit: The design of this project page references the project pages of <a
            href="https://www.matthewtancik.com/nerf">NeRF</a>, <a
            href="https://github.com/DeepMotionEditing/DeepMotionEditing.github.io">DeepMotionEditing</a>, and <a
            href="https://www.lerf.io/">LERF</a>.</p>
</body>